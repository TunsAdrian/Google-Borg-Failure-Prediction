{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-16T13:14:39.097644Z",
     "start_time": "2023-06-16T13:14:27.631898400Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "dataset_root_path = \"D:\\\\Documents\\\\Programming\\\\Python-Projects\\\\Clusterdata_2019_e\\\\\"\n",
    "spark = SparkSession.builder.appName('Failure Prediction on Google Borg Cluster Traces').master('local[*]').getOrCreate()\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "SparkContext.setSystemProperty('spark.driver.memory', '2g')\n",
    "\n",
    "collection_events_df = spark.read.parquet(dataset_root_path + \"collection_events-*.parquet.gz\")\n",
    "instance_events_df = spark.read.parquet(dataset_root_path + \"instance_events-*.parquet.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "5244061"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_events_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T13:14:39.849965500Z",
     "start_time": "2023-06-16T13:14:39.099685400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "1523447380"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_events_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T13:14:41.109733900Z",
     "start_time": "2023-06-16T13:14:39.849965500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# drop unused columns\n",
    "collection_events_df = collection_events_df.drop(*['missing_type', 'alloc_collection_id', 'user', 'collection_name', 'collection_logical_name', 'start_after_collection_ids', 'max_per_machine', 'max_per_switch', 'vertical_scaling', 'scheduler'])\n",
    "instance_events_df = instance_events_df.drop(*['missing_type', 'alloc_collection_id', 'instance_index', 'machine_id', 'alloc_instance_index', 'constraint'])\n",
    "instance_events_df = instance_events_df.na.drop(subset=[\"resource_request\"])\n",
    "\n",
    "# filter only the Jobs from the collections and the Tasks from the instances\n",
    "collection_events_df = collection_events_df.filter(collection_events_df.collection_type == 0)\n",
    "instance_events_df = instance_events_df.filter(instance_events_df.collection_type == 0)\n",
    "\n",
    "# drop columns previously used for filter\n",
    "collection_events_df = collection_events_df.drop(\"collection_type\")\n",
    "instance_events_df = instance_events_df.drop(\"collection_type\")\n",
    "\n",
    "# filter the jobs in the timeframe (between 0 and MAXINT)\n",
    "collection_events_df = collection_events_df.filter((collection_events_df.time != 0) & (collection_events_df.time != (2 ^ 63 - 1)))\n",
    "\n",
    "# filter the job ids out of the timeframe and collect to list\n",
    "task_ids_to_remove_list = collection_events_df.filter((collection_events_df.time == 0) | (collection_events_df.time == (2 ^ 63 - 1))).rdd.map(lambda x: x.collection_id).collect()\n",
    "\n",
    "# filter the tasks in the timeframe (between 0 and MAXINT), that are not part of the jobs that are outside the timeframe\n",
    "instance_events_df = instance_events_df.filter((instance_events_df.collection_id.isin(task_ids_to_remove_list) == False) & (instance_events_df.time != 0) & (instance_events_df.time != (2 ^ 63 - 1)))\n",
    "\n",
    "# remove time column after it was ensured that the records are in the timeframe\n",
    "collection_events_df = collection_events_df.drop(\"time\")\n",
    "instance_events_df = instance_events_df.drop(\"time\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T14:39:23.252262800Z",
     "start_time": "2023-06-15T14:39:17.390097200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: long (nullable = true)\n",
      " |-- collection_id: long (nullable = true)\n",
      " |-- scheduling_class: long (nullable = true)\n",
      " |-- priority: long (nullable = true)\n",
      " |-- parent_collection_id: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- type: long (nullable = true)\n",
      " |-- collection_id: long (nullable = true)\n",
      " |-- scheduling_class: long (nullable = true)\n",
      " |-- priority: long (nullable = true)\n",
      " |-- resource_request: struct (nullable = true)\n",
      " |    |-- cpus: double (nullable = true)\n",
      " |    |-- memory: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collection_events_df.printSchema()\n",
    "instance_events_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T14:39:23.267151200Z",
     "start_time": "2023-06-15T14:39:23.252262800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "5029695"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_events_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T14:39:23.657000200Z",
     "start_time": "2023-06-15T14:39:23.267151200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "1517629860"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_events_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T14:40:13.320205900Z",
     "start_time": "2023-06-15T14:39:23.657000200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- collection_id: long (nullable = true)\n",
      " |-- type: long (nullable = true)\n",
      " |-- scheduling_class: long (nullable = true)\n",
      " |-- priority: long (nullable = true)\n",
      " |-- parent_collection_id: long (nullable = true)\n",
      " |-- cpus: double (nullable = true)\n",
      " |-- memory: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract max cpu and max memory request for each collection_id, from the tasks\n",
    "max_resource_request_df = instance_events_df.groupBy('collection_id').agg(\n",
    "    F.max(F.col('resource_request.cpus')).alias('cpus'),\n",
    "    F.max(F.col('resource_request.memory')).alias('memory'))\n",
    "\n",
    "# add the maximum cpu/memory request to the jobs\n",
    "collection_events_df = collection_events_df.join(max_resource_request_df, on='collection_id')\n",
    "collection_events_df.printSchema()\n",
    "\n",
    "max_resource_request_df = max_resource_request_df.unpersist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T14:40:13.390528100Z",
     "start_time": "2023-06-15T14:40:13.320205900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# take the cpu and memory requests out of the struct\n",
    "instance_events_df = instance_events_df.withColumn('cpus', instance_events_df[\"resource_request.cpus\"]).withColumn('memory', instance_events_df[\"resource_request.memory\"])\n",
    "instance_events_df = instance_events_df.drop(\"resource_request\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T14:40:13.390528100Z",
     "start_time": "2023-06-15T14:40:13.381863800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+--------+--------------------+-----------------+---------------+-------------+\n",
      "|collection_id|scheduling_class|priority|parent_collection_id|             cpus|         memory|event_success|\n",
      "+-------------+----------------+--------+--------------------+-----------------+---------------+-------------+\n",
      "|  35288547244|               2|     360|                null|0.024749755859375|0.0072021484375|            0|\n",
      "+-------------+----------------+--------+--------------------+-----------------+---------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------+----------------+--------+----+------------------+-------------+\n",
      "|collection_id|scheduling_class|priority|cpus|            memory|event_success|\n",
      "+-------------+----------------+--------+----+------------------+-------------+\n",
      "| 237301563343|               2|     210| 0.0|1.9073486328125E-4|            0|\n",
      "+-------------+----------------+--------+----+------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "allowedEventTypes = [4, 5, 6, 7, 8]\n",
    "failureEventTypes = [4, 5, 7, 8]\n",
    "\n",
    "collection_events_df = collection_events_df.filter(collection_events_df.type.isin(allowedEventTypes))\n",
    "collection_events_df = collection_events_df.withColumn(\"event_success\", F.when(collection_events_df.type.isin(failureEventTypes), 0).otherwise(1))\n",
    "collection_events_df = collection_events_df.drop(*['type'])\n",
    "collection_events_df.show(n=1)\n",
    "\n",
    "instance_events_df = instance_events_df.filter(instance_events_df.type.isin(allowedEventTypes))\n",
    "instance_events_df = instance_events_df.withColumn(\"event_success\", F.when(instance_events_df.type.isin(failureEventTypes), 0).otherwise(1))\n",
    "instance_events_df = instance_events_df.drop(*['type'])\n",
    "instance_events_df.show(n=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T14:41:53.517955Z",
     "start_time": "2023-06-15T14:40:13.381863800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "collection_events_df.write.parquet(\"./intermediary_data/jobs_with_type.parquet\")\n",
    "instance_events_df.write.parquet(\"./intermediary_data/tasks_with_type.parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T14:45:17.107877700Z",
     "start_time": "2023-06-15T14:41:53.520459400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
