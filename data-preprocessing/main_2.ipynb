{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-16T12:17:17.394218400Z",
     "start_time": "2023-06-16T12:16:49.720027300Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "dataset_root_path = \"D:\\\\Documents\\\\Programming\\\\Python-Projects\\\\Clusterdata_2019_e\\\\\"\n",
    "spark = SparkSession.builder.appName('Failure Prediction on Google Borg Cluster Traces').master('local[*]').getOrCreate()\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "SparkContext.setSystemProperty('spark.driver.memory', '2g')\n",
    "\n",
    "jobs_with_type_df = spark.read.parquet(\"./intermediary_data/jobs_with_type.parquet\")\n",
    "instance_usage_df = spark.read.parquet(dataset_root_path + \"instance_usage-*.parquet.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "727512"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_jobs_df = jobs_with_type_df.filter(jobs_with_type_df.event_success == 0).select('collection_id', 'parent_collection_id')\n",
    "failed_jobs_df = failed_jobs_df.fillna(0, subset=[\"parent_collection_id\"])\n",
    "failed_jobs_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T12:17:18.558403300Z",
     "start_time": "2023-06-16T12:17:17.394218400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph using NetworkX\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# Add edges to the graph based on the parent-child relationships in the DataFrame\n",
    "edges = failed_jobs_df.select(\"parent_collection_id\", \"collection_id\").collect()\n",
    "graph.add_edges_from(edges)\n",
    "\n",
    "print(nx.is_directed_acyclic_graph(graph))\n",
    "print(nx.dag_longest_path_length(graph) - 1) # because where parent_collection_id = 0 or the node has no ancestors, the edge will be (-no_ancestor-, collection_id), which should not be counted, as these represent top level collections"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T12:17:25.040105300Z",
     "start_time": "2023-06-16T12:17:18.558403300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topological sort length:  726923\n",
      "Failed job ids with depth of 0:  720074\n",
      "Failed job ids with depth of 1:  4983\n",
      "Failed job ids with depth of 2:  1866\n",
      "All failed job ids length:  726923\n"
     ]
    }
   ],
   "source": [
    "topological_sort = list(nx.topological_sort(nx.line_graph(graph)))\n",
    "print('Topological sort length: ', len(topological_sort))\n",
    "\n",
    "# take as depth 0 the jobs that don't have a parent (are top level jobs), or the ones that do have a parent that did not fail (not appearing in the list, therefore the collection_id doesn't have ancestors)\n",
    "failed_jobs_depth_0 = list(filter(lambda pair: (pair[0] == 0) or (pair[0] != 0 and len(nx.ancestors(graph, pair[0])) == 0), topological_sort))\n",
    "\n",
    "failed_job_ids_depth_0 = set()\n",
    "for job in failed_jobs_depth_0:\n",
    "    failed_job_ids_depth_0.add(job[1])\n",
    "\n",
    "print('Failed job ids with depth of 0: ', len(failed_job_ids_depth_0))\n",
    "failed_jobs_with_depth_bigger_than_0 = list((set(topological_sort) - set(failed_jobs_depth_0)))\n",
    "\n",
    "failed_jobs_depth_1 = list(filter(lambda pair: pair[0] in failed_job_ids_depth_0, failed_jobs_with_depth_bigger_than_0))\n",
    "failed_job_ids_depth_1 = set()\n",
    "for job in failed_jobs_depth_1:\n",
    "    failed_job_ids_depth_1.add(job[1])\n",
    "\n",
    "print('Failed job ids with depth of 1: ', len(failed_job_ids_depth_1))\n",
    "failed_jobs_with_depth_bigger_than_1 = list((set(failed_jobs_with_depth_bigger_than_0) - set(failed_jobs_depth_1)))\n",
    "\n",
    "failed_jobs_depth_2 = list(filter(lambda pair: pair[0] in failed_job_ids_depth_1, failed_jobs_with_depth_bigger_than_1))\n",
    "failed_job_ids_depth_2 = set()\n",
    "for job in failed_jobs_depth_2:\n",
    "    failed_job_ids_depth_2.add(job[1])\n",
    "\n",
    "print('Failed job ids with depth of 2: ', len(failed_job_ids_depth_2))\n",
    "\n",
    "failed_jobs_ids = failed_job_ids_depth_0.union(failed_job_ids_depth_1).union(failed_job_ids_depth_2)\n",
    "print('All failed job ids length: ', len(failed_jobs_ids))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T12:17:28.708348600Z",
     "start_time": "2023-06-16T12:17:25.040105300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- collection_id: long (nullable = true)\n",
      " |-- parent_collection_id: long (nullable = true)\n",
      " |-- collection_end_time: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# get only the valid failed jobs\n",
    "df = spark.createDataFrame(failed_jobs_ids, LongType())\n",
    "failed_jobs_df = failed_jobs_df.join(df, failed_jobs_df.collection_id == df.value, how=\"leftsemi\")\n",
    "\n",
    "# filter only the Job instance usage entries and select the maximum end_time for each collection_id\n",
    "instance_usage_df = instance_usage_df.filter(instance_usage_df.collection_type == 0).select('end_time', 'collection_id')\n",
    "instance_usage_df = instance_usage_df.groupBy(\"collection_id\").agg(F.max(\"end_time\").alias(\"collection_end_time\"))\n",
    "\n",
    "failed_jobs_df = failed_jobs_df.join(instance_usage_df, on='collection_id')\n",
    "failed_jobs_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T12:17:29.652271700Z",
     "start_time": "2023-06-16T12:17:28.708348600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# create a dataframe with failed jobs of depth 1\n",
    "df = spark.createDataFrame(failed_job_ids_depth_1, LongType())\n",
    "failed_jobs_depth_1_df = failed_jobs_df.join(df, failed_jobs_df.collection_id == df.value, how='leftsemi')\n",
    "failed_jobs_depth_1_df = failed_jobs_depth_1_df.withColumnRenamed('collection_id', 'collection_id_1').withColumnRenamed('parent_collection_id', 'parent_collection_id_1').withColumnRenamed('collection_end_time', 'collection_end_time_1')\n",
    "\n",
    "# filter jobs that died after their parent died, because they were killed by the parent and are not relevant for the analysis\n",
    "failed_jobs_from_parent_depth_1_df = failed_jobs_depth_1_df.alias(\"c\").join(failed_jobs_df.alias(\"p\"), F.col(\"c.parent_collection_id_1\") == F.col(\"p.collection_id\"), \"inner\").filter(F.col(\"c.collection_end_time_1\") >= F.col(\"p.collection_end_time\"))\n",
    "failed_jobs_from_parent_depth_1_df = failed_jobs_from_parent_depth_1_df.drop(*['collection_id', 'parent_collection_id', 'collection_end_time'])\n",
    "failed_jobs_from_parent_depth_1_df = failed_jobs_from_parent_depth_1_df.withColumnRenamed('collection_id_1', 'collection_id').withColumnRenamed('parent_collection_id_1', 'parent_collection_id').withColumnRenamed('collection_end_time_1', 'collection_end_time')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T12:17:29.715182100Z",
     "start_time": "2023-06-16T12:17:29.652271700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# create a dataframe with failed jobs of depth 2\n",
    "df = spark.createDataFrame(failed_job_ids_depth_2, LongType())\n",
    "failed_jobs_depth_2_df = failed_jobs_df.join(df, failed_jobs_df.collection_id == df.value, how='leftsemi')\n",
    "failed_jobs_depth_2_df = failed_jobs_depth_2_df.withColumnRenamed('collection_id', 'collection_id_2').withColumnRenamed('parent_collection_id', 'parent_collection_id_2').withColumnRenamed('collection_end_time', 'collection_end_time_2')\n",
    "\n",
    "# filter jobs that died after their parent died, because they were killed by the parent and are not relevant for the analysis\n",
    "failed_jobs_from_parent_depth_2_df = failed_jobs_depth_2_df.alias(\"c\").join(failed_jobs_depth_1_df.alias(\"p\"), F.col(\"c.parent_collection_id_2\") == F.col(\"p.collection_id_1\"), \"inner\").filter(F.col(\"c.collection_end_time_2\") >= F.col(\"p.collection_end_time_1\"))\n",
    "failed_jobs_from_parent_depth_2_df = failed_jobs_from_parent_depth_2_df.drop(*['collection_id_1', 'parent_collection_id_1', 'collection_end_time_1'])\n",
    "failed_jobs_from_parent_depth_2_df = failed_jobs_from_parent_depth_2_df.withColumnRenamed('collection_id_2', 'collection_id').withColumnRenamed('parent_collection_id_2', 'parent_collection_id').withColumnRenamed('collection_end_time_2', 'collection_end_time')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T12:17:29.780193200Z",
     "start_time": "2023-06-16T12:17:29.718254700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- collection_id: long (nullable = true)\n",
      " |-- parent_collection_id: long (nullable = true)\n",
      " |-- collection_end_time: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- collection_id: long (nullable = true)\n",
      " |-- parent_collection_id: long (nullable = true)\n",
      " |-- collection_end_time: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "failed_jobs_from_parent_depth_1_df.printSchema()\n",
    "failed_jobs_from_parent_depth_2_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T12:17:29.795928Z",
     "start_time": "2023-06-16T12:17:29.780193200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "failed_jobs_from_parent = failed_jobs_from_parent_depth_1_df.union(failed_jobs_from_parent_depth_2_df)\n",
    "failed_jobs_from_parent = failed_jobs_from_parent.drop_duplicates()\n",
    "failed_jobs_from_parent = failed_jobs_from_parent.select(\"collection_id\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T12:17:29.828418800Z",
     "start_time": "2023-06-16T12:17:29.795928Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- collection_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "failed_jobs_from_parent.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T12:17:29.843668700Z",
     "start_time": "2023-06-16T12:17:29.828418800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "failed_jobs_from_parent.write.parquet(\"./intermediary_data/job_ids_to_remove.parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-16T12:23:49.340583Z",
     "start_time": "2023-06-16T12:17:29.843668700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
